# Implementation Plan: Optional Dataplex Integration

## Overview
This plan details the implementation of an optional Dataplex integration into the existing SQL agent. The integration will leverage semantic metadata from Dataplex to generate more accurate and context-aware SQL queries. The feature will be configurable and disabled by default.

## Archon Project
All tasks for this implementation plan should be created and managed within the existing Archon project: **"ADK SQL Generation Agent"**.

## Execution Strategy
This plan should be executed with a strong emphasis on research and iterative validation.

-   **Archon-First Approach:** Before starting any task, consult the Archon knowledge base using `rag_search_knowledge_base` and `rag_search_code_examples`. The knowledge base contains information on `pytest`, `ADK`, `uv`, `poethepoet`, `honcho`, `MCP Toolbox`, and more. This will ensure we follow best practices and existing patterns.
-   **MCP Server Utilization:** Leverage the available MCP servers. The `Dataplex` and `BigQuery` servers will be used to inspect data and validate changes. The `Terraform` server will be used as a documentation and reference source to ensure the correct use of providers and resources in the IaC scripts.
-   **Iterative Validation:** Each task includes a "Validation" step. This step **must** be completed before moving to the next task. This creates a tight feedback loop and ensures each component is working correctly before building on top of it.

## Requirements Summary
- **FR1:** Configure a new `dataplex` source in `tools.yaml`.
- **FR2:** Control Dataplex integration with a `DATAPLEX_ENABLED` environment variable.
- **FR3:** The new `SemanticEnricher` agent must have access to Dataplex MCP tools.
- **FR4:** Create a `TermExtractor` agent to extract business concepts from the user query.
- **FR5:** Create a `DataplexSearcher` agent to find semantically related data assets.
- **FR6:** The `DataplexSearcher` agent will also look up and consolidate metadata.
- **FR7:** The `sql_generator_loop` must be updated to use the semantic context.
- **FR8:** The `schema_inspector` must be updated to use a pre-filtered list of tables.
- **FR9:** A new `SemanticEnricher` sequential agent will orchestrate the new workflow.
- **FR10-12:** A new BigQuery dataset and Dataplex configuration will be created using Terraform, with data generated by a script.
- **AS3:** Use `poethepoet` to automate the data generation script.
- **DR1:** Update `README.md` with instructions for the new feature.

## Research Findings
### Best Practices
- **Conditional Agent Injection:** The most effective way to implement the optionality is to conditionally insert the `SemanticEnricher` agent into the main `SequentialAgent`'s sub-agent list at runtime based on the `DATAPLEX_ENABLED` environment variable. This avoids code duplication and maintains a clean separation of concerns.
- **Centralized Configuration:** All new environment variables (`DATAPLEX_ENABLED`, `DATAPLEX_LOCATION`, `DATAPLEX_LAKE_ID`, `DATAPLEX_ZONE_ID`) should be managed in the `agents/sql_agent/config.py` file for easy access and consistency.
- **Terraform for IaC:** Using Terraform to manage all Google Cloud resources (BigQuery, Dataplex, IAM) is the best practice for ensuring a repeatable and automated setup.

### Reference Implementations
- The existing structure of `agents/sql_agent/agent.py` provides a clear pattern for how to define and orchestrate `SequentialAgent`s.
- The implementation of `schema_inspector.py` serves as a good reference for creating new agents that use the `McpToolset`.

### Technology Decisions
- **Terraform:** For Infrastructure as Code to manage all Google Cloud resources.
- **Python (`Faker`, `pandas`):** For generating realistic sample data.
- **`poethepoet`:** To create a simple CLI for running the data generation script.

## Implementation Tasks

### Phase 1: Environment Setup & Configuration
1.  **Task: Create Terraform Scripts for Infrastructure**
    -   Description: Develop Terraform scripts to provision the BigQuery dataset, tables, Dataplex Lake, Zone, Assets, and Data Catalog Tag Templates.
    -   Files to create: `terraform/main.tf`, `terraform/variables.tf`, `terraform/outputs.tf`, `terraform/bigquery.tf`, `terraform/dataplex.tf`, `terraform/iam.tf`
    -   Dependencies: None
    -   Validation: After writing the scripts, use the Terraform MCP server (e.g., `search_providers`, `get_provider_details`) to look up documentation for the `google` provider and the specific resources being used (e.g., `google_bigquery_dataset`, `google_dataplex_lake`) to ensure correctness. Run `terraform plan` locally to verify the changes. After applying, use the BigQuery and Dataplex MCP tools (e.g., `list_tables`, `lookup_entry`) to confirm that all resources have been created correctly.

2.  **Task: Create Data Generation Script**
    -   Description: Develop a Python script to generate sample data for the BigQuery tables using Faker and pandas.
    -   Files to create: `scripts/datagen.py`
    -   Dependencies: Terraform infrastructure must be defined.
    -   Validation: Run the script and inspect the generated CSV files in the `tmp/` directory to ensure they have the correct format and data integrity.

3.  **Task: Configure `poethepoet`**
    -   Description: Add `poethepoet` as a dev dependency and configure a `datagen` task in `pyproject.toml`.
    -   Files to modify: `pyproject.toml`
    -   Dependencies: Data generation script must be created.
    -   Validation: Run `uv run poe datagen` and confirm that it executes the data generation script without errors.

4.  **Task: Update `tools.yaml`**
    -   Description: Add the new `dataplex` source and the required Dataplex tools (`dataplex-search-entries`, `dataplex-lookup-entry`, `dataplex-search-aspect-types`).
    -   Files to modify: `tools.yaml`
    -   Dependencies: None
    -   Validation: Restart the MCP server and use `mcp-cli` or a simple script to verify that the new Dataplex tools are available and responsive.

### Phase 2: Core Agent Implementation
1.  **Task: Create `TermExtractor` Agent**
    -   Description: Create a new `LlmAgent` that extracts key business terms from the user's query. **The agent must use the prompt instructions as defined in FR4 of the PRD.**
    -   Files to create: `agents/sql_agent/semantic_enricher.py` (will contain all new agents)
    -   Dependencies: None
    -   Validation: Write a unit test for the agent that provides a sample user query and asserts that the agent correctly extracts the expected business terms into the session state.

2.  **Task: Create `DataplexSearcher` Agent**
    -   Description: Create a new `LlmAgent` that uses the extracted terms to search Dataplex and retrieve semantic context. **The agent must use the system prompt defined in Appendix D.1 of the PRD.**
    -   Files to create: `agents/sql_agent/semantic_enricher.py`
    -   Dependencies: `TermExtractor` agent.
    -   Validation: Write a unit test that mocks the Dataplex MCP tool calls and verifies that the agent correctly processes the mocked responses and populates the session state with `semantic_context` and `filtered_table_list`.

3.  **Task: Create `SemanticEnricher` Agent**
    -   Description: Create a new `SequentialAgent` that orchestrates the `TermExtractor` and `DataplexSearcher` agents.
    -   Files to create: `agents/sql_agent/semantic_enricher.py`
    -   Dependencies: `TermExtractor` and `DataplexSearcher` agents.
    -   Validation: Write an integration test for this sub-sequence that takes a user query and verifies that the final session state contains the expected semantic context and table list.

### Phase 3: Integration & Testing
1.  **Task: Conditionally Add `SemanticEnricher` to Root Agent**
    -   Description: Modify the root `sql_agent` to conditionally insert the `SemanticEnricher` agent based on the `DATAPLEX_ENABLED` environment variable.
    -   Files to modify: `agents/sql_agent/agent.py`, `agents/sql_agent/config.py`
    -   Dependencies: `SemanticEnricher` agent.
    -   Validation: Manually test the agent with `DATAPLEX_ENABLED=true` and `DATAPLEX_ENABLED=false`. Confirm the `SemanticEnricher` agent is only active when enabled by checking the agent's execution trace or logs.

2.  **Task: Update `schema_inspector`**
    -   Description: Modify the `schema_inspector` to dynamically fetch schema based on `filtered_table_list` from session state when Dataplex is enabled. **Remove the hardcoded logic for `bigquery-public-data.google_trends`**. If Dataplex is disabled, it should revert to fetching the schema for `bigquery-public-data.google_trends`.
    -   Files to modify: `agents/sql_agent/schema_inspector.py`
    -   Dependencies: `SemanticEnricher` agent.
    -   Validation: Run the full agent with Dataplex enabled. Verify from the logs that the `schema_inspector` is only fetching schemas for the tables identified by the `DataplexSearcher`. Then, run with Dataplex disabled and verify it fetches the `google_trends` schema.

3.  **Task: Update `sql_generator_loop`**
    -   Description: Modify the `generator` agent within the loop to incorporate the `semantic_context` from the session state into its prompt. **The `semantic_context` should be added as a new, clearly-delineated "Semantic Context" section to the `GENERATOR_SYSTEM_PROMPT` in `prompts.py`, which will be conditionally populated.**
    -   Files to modify: `agents/sql_agent/sql_generator_loop.py`, `agents/sql_agent/prompts.py`
    -   Dependencies: `SemanticEnricher` agent.
    -   Validation: Run an end-to-end test with a query that requires semantic context (e.g., "total cost of refunds"). Verify that the generated SQL correctly incorporates the business rule (`+ 15.00`) from the `semantic_context`.

4.  **Task: Update Documentation**
    -   Description: Update the `README.md` with instructions for setting up and running the Dataplex-enabled agent.
    -   Files to modify: `README.md`
    -   Dependencies: All other tasks.
    -   Validation: Request manual validation from the user: "Please review the updated README.md. Does it clearly explain how to set up and run the Dataplex-enabled feature? Please confirm to proceed."

## Codebase Integration Points
### Files to Modify
-   `tools.yaml`: Add new `dataplex` source and tools.
-   `pyproject.toml`: Add `poethepoet` and data generation dependencies.
-   `agents/sql_agent/config.py`: Add new environment variables.
-   `agents/sql_agent/agent.py`: Conditionally add the new `SemanticEnricher` agent.
-   `agents/sql_agent/schema_inspector.py`: Update to use the filtered table list.
-   `agents/sql_agent/sql_generator_loop.py`: Update the generator prompt to use semantic context.
-   `agents/sql_agent/prompts.py`: Add new prompt instructions for the generator.
-   `README.md`: Add documentation for the new feature.

### New Files to Create
-   `agents/sql_agent/semantic_enricher.py`: Will contain the new `TermExtractor`, `DataplexSearcher`, and `SemanticEnricher` agents.
-   `scripts/datagen.py`: The data generation script.
-   `terraform/`: A new directory to hold all Terraform IaC files.

### Existing Patterns to Follow
-   The creation of new agents should follow the pattern in `schema_inspector.py`, using `LlmAgent` and `McpToolset`.
-   The orchestration of agents should follow the pattern in `agent.py`, using `SequentialAgent`.

## Technical Design

### Architecture Diagram
```
[User Query] -> [SemanticEnricher (if enabled)] -> [SchemaInspector] -> [SqlGeneratorLoop] -> [FinalResponder] -> [Answer]

SemanticEnricher:
[User Query] -> [TermExtractor] -> [extracted_terms] -> [DataplexSearcher] -> [semantic_context, filtered_table_list]
```

### Data Flow
1.  The user provides a natural language query.
2.  If `DATAPLEX_ENABLED=true`, the `SemanticEnricher` agent runs first.
3.  The `TermExtractor` extracts business terms from the query and saves them to `session.state['extracted_terms']`.
4.  The `DataplexSearcher` uses these terms to search Dataplex, finds relevant tables, and looks up their metadata.
5.  The `DataplexSearcher` saves the consolidated metadata to `session.state['semantic_context']` and the list of relevant tables to `session.state['filtered_table_list']`.
6.  The `SchemaInspector` reads `session.state['filtered_table_list']` and fetches the schema only for those tables, saving it to `session.state['schema']`.
7.  The `SqlGeneratorLoop` reads `session.state['schema']` and `session.state['semantic_context']` to generate a semantically correct SQL query.
8.  The rest of the flow continues as before.

## Dependencies and Libraries
-   `poethepoet`: For task running.
-   `Faker`: For data generation.
-   `pandas`: For data manipulation in the generation script.

## Testing Strategy
-   Unit tests will be created for the new agents, mocking the `McpToolset` to return sample Dataplex responses.
-   Integration tests will be run against a live, Terraform-provisioned environment to validate the end-to-end flow using the "gold standard" queries from the PRD.

## Success Criteria
-   [ ] All functional requirements from the PRD are met.
-   [ ] The Dataplex integration is disabled by default and can be enabled via an environment variable.
-   [ ] The agent can successfully use Dataplex metadata to generate semantically correct SQL for the test cases in the PRD.
-   [ ] The Terraform scripts successfully provision all required infrastructure.
-   [ ] The data generation script successfully populates the BigQuery tables.
-   [ ] The `README.md` is updated with clear instructions.

## Notes and Considerations
-   The quality of the Dataplex metadata is critical to the success of this feature. The Terraform scripts must accurately create the required tags and aspects.
-   The prompts for the new LLM agents (`TermExtractor`, `DataplexSearcher`) will need to be carefully crafted and tested to ensure they perform their tasks reliably.
-   Error handling will be important, especially for the Dataplex API calls. The agents should be resilient to missing metadata or failed API calls.

---
*This plan is ready for execution with `/archon:execute-plan`*
